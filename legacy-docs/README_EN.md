<div align="center">
  <img src="./images/logo.png"alt="VideoCaptioner Logo" width="100">
  <p>Kaka Subtitle Assistant</p>
  <h1>VideoCaptioner</h1>
  <p>An LLM-powered video subtitle processing assistant, supporting speech recognition, subtitle segmentation, optimization, and translation.</p>

  [ÁÆÄ‰Ωì‰∏≠Êñá](../README.md) / [Ê≠£È´î‰∏≠Êñá](./README_TW.md) / English / [Êó•Êú¨Ë™û](./README_JA.md)

</div>

## üìñ Introduction

Kaka Subtitle Assistant (VideoCaptioner) is easy to operate and doesn't require high-end hardware. It supports both online API calls and local offline processing (with GPU support) for speech recognition. It leverages Large Language Models (LLMs) for intelligent subtitle segmentation, correction, and translation. It offers a one-click solution for the entire video subtitle workflow! Add stunning subtitles to your videos.

- Support for word-level timestamps and VAD voice activity detection with high recognition accuracy
- LLM-based semantic understanding to automatically reorganize word-by-word subtitles into natural, fluent sentence paragraphs
- Context-aware AI translation with reflection optimization mechanism for idiomatic and professional translations
- Batch video subtitle synthesis support to improve processing efficiency
- Intuitive subtitle editing and viewing interface with real-time preview and quick editing

## üì∏ Interface Preview

<div align="center">
  <img src="https://h1.appinn.me/file/1731487405884_main.png" alt="Software Interface Preview" width="90%" style="border-radius: 5px;">
</div>

![Page Preview](https://h1.appinn.me/file/1731487410170_preview1.png)
![Page Preview](https://h1.appinn.me/file/1731487410832_preview2.png)


## üß™ Testing

Processing a 14-minute 1080P [English TED video from Bilibili](https://www.bilibili.com/video/BV1jT411X7Dz) end-to-end, using the local Whisper model for speech recognition and the `gpt-5-mini` model for optimization and translation into Chinese, took approximately **4 minutes**.

Based on backend calculations, the cost for model optimization and translation was less than ¬•0.01 (calculated using OpenAI's official pricing).

For detailed results of subtitle and video synthesis, please refer to the [TED Video Test](./test.md).


## üöÄ Quick Start

### For Windows Users

The software is lightweight, with a package size of less than 60MB, and includes all necessary environments. Download and run directly.

1. Download the latest version of the executable from the [Release](https://github.com/WEIFENG2333/VideoCaptioner/releases) page. Or: [Lanzou Cloud Download](https://wwwm.lanzoue.com/ii14G2pdsbej)

2. Open the installer to install.

3. LLM API Configuration (for subtitle segmentation and correction), you can use [this project's API relay](https://api.videocaptioner.cn)

4. Translation configuration, choose whether to enable translation (default uses Microsoft Translator, average quality, recommend configuring your own API KEY for LLM translation)

5. Speech recognition configuration (default uses B interface for online speech recognition, use local transcription for languages other than Chinese and English)

### For macOS Users

#### One-Click Install & Run (Recommended)

```bash
# Method 1: Direct run (auto-installs uv, clones project, installs dependencies)
curl -fsSL https://raw.githubusercontent.com/WEIFENG2333/VideoCaptioner/main/scripts/run.sh | bash

# Method 2: Clone first, then run
git clone https://github.com/WEIFENG2333/VideoCaptioner.git
cd VideoCaptioner
./scripts/run.sh
```

The script will automatically:

1. Install the [uv](https://docs.astral.sh/uv/) package manager (if not installed)
2. Clone the project to `~/VideoCaptioner` (if not running from project directory)
3. Install all Python dependencies
4. Launch the application

<details>
<summary>Manual Installation Steps</summary>

#### 1. Install uv package manager

```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
```

#### 2. Install system dependencies (macOS)

```bash
brew install ffmpeg
```

#### 3. Clone and run

```bash
git clone https://github.com/WEIFENG2333/VideoCaptioner.git
cd VideoCaptioner
uv sync          # Install dependencies
uv run python main.py  # Run
```

</details>

### Developer Guide

```bash
# Install dependencies (including dev dependencies)
uv sync

# Run application
uv run python main.py

# Type checking
uv run pyright

# Code linting
uv run ruff check .
```

## ‚ú® Main Features

The software fully utilizes the advantages of Large Language Models (LLMs) in understanding context to further process subtitles generated by speech recognition. It effectively corrects typos, unifies terminology, and makes the subtitle content more accurate and coherent, providing users with an excellent viewing experience!

#### 1. Multi-platform Video Download and Processing
- Supports mainstream video platforms (Bilibili, YouTube, TikTok, X, etc.)
- Automatically extracts and processes the original subtitles of the video.

#### 2. Professional Speech Recognition Engine
- Provides multiple online recognition interfaces with effects comparable to Jianying (free, high-speed).
- Supports local Whisper model (privacy protection, offline).

#### 3. Intelligent Subtitle Correction
- Automatically optimizes the format of terminology, code snippets, and mathematical formulas.
- Contextual sentence segmentation optimization to improve reading experience.
- Supports manuscript prompts, using original manuscripts or related prompts to optimize subtitle segmentation.

#### 4. High-Quality Subtitle Translation
- Context-aware intelligent translation ensures that the translation takes the entire text into account.
- Guides the large model to reflect on the translation through prompts, improving translation quality.
- Uses a sequence fuzzy matching algorithm to ensure complete consistency of the timeline.

#### 5. Subtitle Style Adjustment
- Rich subtitle style templates (popular science style, news style, anime style, etc.).
- Multiple subtitle video formats (SRT, ASS, VTT, TXT).


## ‚öôÔ∏è Basic Configuration

### 1. LLM API Configuration Instructions

LLM is used for subtitle segmentation, optimization, and translation (if LLM translation is selected).

| Configuration Item | Description |
|--------|------|
| SiliconCloud | [SiliconCloud Official](https://cloud.siliconflow.cn/i/onCHcaDx), for configuration see [online docs](https://weifeng2333.github.io/VideoCaptioner/config/llm)<br>Low concurrency, recommend setting threads below 5. |
| DeepSeek | [DeepSeek Official](https://platform.deepseek.com), recommend using `deepseek-v3` model. |
| OpenAI Compatible | If you have API from other providers, fill in directly. base_url and api_key [VideoCaptioner API](https://api.videocaptioner.cn) |

Note: If your API provider doesn't support high concurrency, lower the "thread count" in settings to avoid request errors.

---

For high concurrency, or to use quality models like OpenAI or Claude for subtitle correction and translation:

Use this project's ‚ú®LLM API Relay‚ú®: [https://api.videocaptioner.cn](https://api.videocaptioner.cn)

Supports high concurrency, excellent value, with many domestic and international models available.

After registering and getting your key, configure settings as follows:

BaseURL: `https://api.videocaptioner.cn/v1`

API-key: `Get from Personal Center - API Token page.`

üí° Model Selection Recommendations (high-value models selected at each quality tier):

- High quality: `gemini-3-pro`, `claude-sonnet-4-5-20250929` (cost ratio: 3)

- Higher quality: `gpt-5-2025-08-07`, `claude-haiku-4-5-20251001` (cost ratio: 1.2)

- Medium quality: `gpt-5-mini`, `gemini-3-flash` (cost ratio: 0.3)

This site supports ultra-high concurrency, max out the thread count in the software~ Processing speed is very fast~

For more detailed API configuration tutorial: [API Configuration](https://weifeng2333.github.io/VideoCaptioner/config/llm)

---

### 2. Translation Configuration

| Configuration Item | Description |
| -------------- | ----------------------------------------------------------------------------------------------------------------------------- |
| LLM Translation | üåü Best translation quality. Uses AI large models for translation, better context understanding, more natural translations. Requires LLM API configuration (e.g., OpenAI, DeepSeek, etc.) |
| Microsoft Translator | Uses Microsoft's translation service, very fast |
| Google Translate | Google's translation service, fast, but requires access to Google's network |

Recommended: `LLM Translation` for the best translation quality.

### 3. Speech Recognition Interface Description

| Interface Name | Supported Languages | Running Mode | Description |
|---------|---------|---------|------|
| Interface B | Chinese, English only | Online | Free, fast |
| Interface J | Chinese, English only | Online | Free, fast |
| WhisperCpp | Chinese, Japanese, Korean, English, and 99 other languages. Good performance for foreign languages. | Local | (Actual use is unstable) Requires downloading transcription models.<br>Chinese: Medium or larger model recommended.<br>English, etc.: Smaller models can achieve good results. |
| fasterWhisper üëç | Chinese, English, and 99 other languages. Excellent performance for foreign languages, more accurate timeline. | Local | (üåüRecommendedüåü) Requires downloading the program and transcription models.<br>Supports CUDA, faster, accurate transcription.<br>Super accurate timestamp subtitles.<br>Windows only |


### 4. Local Whisper Speech Recognition Configuration (Requires download within the software)

There are two Whisper versions: WhisperCpp and fasterWhisper (recommended). The latter has better performance and both require downloading models within the software.

| Model | Disk Space | RAM Usage | Description |
|------|----------|----------|------|
| Tiny | 75 MiB | ~273 MB | Transcription is mediocre, for testing only. |
| Small | 466 MiB | ~852 MB | English recognition is already good. |
| Medium | 1.5 GiB | ~2.1 GB | This version is recommended as the minimum for Chinese recognition. |
| Large-v2 üëç | 2.9 GiB | ~3.9 GB | Good performance, recommended if your configuration allows. |
| Large-v3 | 2.9 GiB | ~3.9 GB | Community feedback suggests potential hallucination/subtitle repetition issues. |

Recommended model: `Large-v2` is stable and of good quality.


### 5. Manuscript Matching

- On the "Subtitle Optimization and Translation" page, there is a "Manuscript Matching" option, which supports the following **one or more** types of content to assist in subtitle correction and translation:

| Type | Description | Example |
|------|------|------|
| Glossary | Correction table for terminology, names, and specific words. | Machine Learning->Êú∫Âô®Â≠¶‰π†<br>Elon Musk->È©¨ÊñØÂÖã<br>Turing patterns<br>Bus paradox |
| Original Subtitle Text | The original manuscript or related content of the video. | Complete speech scripts, lecture notes, etc. |
| Correction Requirements | Specific correction requirements related to the content. | Unify personal pronouns, standardize terminology, etc.<br>Fill in requirements **related to the content**, [example reference](https://github.com/WEIFENG2333/VideoCaptioner/issues/59#issuecomment-2495849752) |

- If you need manuscript assistance for subtitle optimization, fill in the manuscript information first, then start the task processing.
- Note: When using small LLM models with limited context, it is recommended to keep the manuscript content within 1000 words. If using a model with a larger context window, you can appropriately increase the manuscript content.


### 6. Cookie Configuration Instructions

If you encounter the following situations when using the URL download function:
1. The video website requires login information to download.
2. Only lower resolution videos can be downloaded.
3. Verification is required when network conditions are poor.

- Please refer to the [Cookie Configuration Instructions](https://weifeng2333.github.io/VideoCaptioner/guide/cookies-config) to obtain cookie information and place the `cookies.txt` file in the `AppData` directory of the software installation directory to download high-quality videos normally.

## üí° Software Process Introduction

The simple processing flow of the program is as follows:
```
Speech Recognition -> Subtitle Segmentation (optional) -> Subtitle Optimization & Translation (optional) -> Subtitle & Video Synthesis
```

The main directory structure of the project is as follows:
```
VideoCaptioner/
‚îú‚îÄ‚îÄ app/                        # Application source code directory
‚îÇ   ‚îú‚îÄ‚îÄ common/                 # Common modules (config, signal bus)
‚îÇ   ‚îú‚îÄ‚îÄ components/             # UI components
‚îÇ   ‚îú‚îÄ‚îÄ core/                   # Core business logic (ASR, translation, optimization, etc.)
‚îÇ   ‚îú‚îÄ‚îÄ thread/                 # Async threads
‚îÇ   ‚îî‚îÄ‚îÄ view/                   # Interface views
‚îú‚îÄ‚îÄ resource/                   # Resource file directory
‚îÇ   ‚îú‚îÄ‚îÄ assets/                 # Icons, Logo, etc.
‚îÇ   ‚îú‚îÄ‚îÄ bin/                    # Binary programs (FFmpeg, Whisper, etc.)
‚îÇ   ‚îú‚îÄ‚îÄ fonts/                  # Font files
‚îÇ   ‚îú‚îÄ‚îÄ subtitle_style/         # Subtitle style templates
‚îÇ   ‚îî‚îÄ‚îÄ translations/           # Multi-language translation files
‚îú‚îÄ‚îÄ work-dir/                   # Working directory (processed videos and subtitles)
‚îú‚îÄ‚îÄ AppData/                    # Application data directory
‚îÇ   ‚îú‚îÄ‚îÄ cache/                  # Cache directory (transcription, LLM requests)
‚îÇ   ‚îú‚îÄ‚îÄ models/                 # Whisper model files
‚îÇ   ‚îú‚îÄ‚îÄ logs/                   # Log files
‚îÇ   ‚îî‚îÄ‚îÄ settings.json           # User settings
‚îú‚îÄ‚îÄ scripts/                    # Installation and run scripts
‚îú‚îÄ‚îÄ main.py                     # Program entry
‚îî‚îÄ‚îÄ pyproject.toml              # Project configuration and dependencies
```

## üìù Notes

1. The quality of subtitle segmentation is crucial for the viewing experience. The software can intelligently reorganize word-by-word subtitles into paragraphs that conform to natural language habits and perfectly synchronize with the video frames.

2. During processing, only the text content is sent to the large language model, without timeline information, which greatly reduces processing overhead.

3. In the translation stage, we adopt the "translate-reflect-translate" methodology proposed by Andrew Ng. This iterative optimization method ensures the accuracy of the translation.

4. When processing YouTube links, video subtitles are automatically downloaded, saving the transcription step and significantly reducing operation time.

## ü§ù Contribution Guidelines

The project is constantly being improved. If you encounter any bugs during use, please feel free to submit [Issues](https://github.com/WEIFENG2333/VideoCaptioner/issues) and Pull Requests to help improve the project.

## üìù Changelog

View the complete update history at [CHANGELOG.md](../CHANGELOG.md)

## ‚≠ê Star History

[![Star History Chart](https://api.star-history.com/svg?repos=WEIFENG2333/VideoCaptioner&type=Date)](https://star-history.com/#WEIFENG2333/VideoCaptioner&Date)

## üíñ Support the Author

If you find this project helpful, please give it a Star!

<details>
<summary>Donation Support</summary>
<div align="center">
  <img src="./images/alipay.jpg" alt="Alipay QR Code" width="30%">
  <img src="./images/wechat.jpg" alt="WeChat QR Code" width="30%">
</div>
</details>
