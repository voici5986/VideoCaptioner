"""LLM-based text splitting tests.

Requires environment variables:
    OPENAI_BASE_URL: OpenAI-compatible API endpoint
    OPENAI_API_KEY: API key for authentication
    OPENAI_MODEL: Model name (optional, defaults to gpt-4o-mini)
"""

import os
from typing import Callable

import pytest

from app.core.split.split_by_llm import count_words, split_by_llm


@pytest.mark.integration
class TestSplitByLLM:
    """Test suite for LLM-based text splitting."""

    def test_count_words_chinese(self):
        """Test word counting for Chinese text."""
        text = "å¤§å®¶å¥½æˆ‘å«æ¨ç‰æºªæ¥è‡ªç¦å»ºå¦é—¨"
        assert count_words(text) == 14  # 14 Chinese characters

    def test_count_words_english(self):
        """Test word counting for English text."""
        text = "Hello world this is a test sentence"
        assert count_words(text) == 7  # 7 English words

    def test_count_words_mixed(self):
        """Test word counting for mixed Chinese and English text."""
        text = "å¤§å®¶å¥½ hello æˆ‘æ˜¯ world"
        # 5 Chinese chars + 2 English words = 7
        assert count_words(text) == 7

    def test_split_chinese_text(self, check_env_vars: Callable):
        """Test splitting Chinese text with LLM."""
        check_env_vars("OPENAI_BASE_URL", "OPENAI_API_KEY")

        text = "å¤§å®¶å¥½æˆ‘å«æ¨ç‰æºªæ¥è‡ªæœ‰ç€è‰¯å¥½éŸ³ä¹æ°›å›´çš„ç¦å»ºå¦é—¨è‡ªè®°äº‹èµ·æˆ‘çœ¼ä¸­çš„ä¸–ç•Œå°±æ˜¯æœ¦èƒ§çš„ç«¥è¯ä¹¦æ˜¯å„è‰²æ‚ä¹±çš„çº¿æ¡ç”µè§†æœºæ˜¯é¢œè‰²å„å¼‚çš„é›ªèŠ±å°ä¼™ä¼´æ˜¯åªå¬å…¶å£°ä¸ä¾¿éª‘è¡Œçš„é©¬èµ›å…‹åæ¥æˆ‘æ‰çŸ¥é“è¿™æ˜¯ä¸€ç§çœ¼åº•é»„æ–‘ç–¾ç—…è™½ä¸è‡³äºå¤±æ˜ä½†ç»ˆèº«æ— æ³•æ²»æ„ˆ"
        model = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
        max_limit = 18

        result = split_by_llm(text, model=model, max_word_count_cjk=max_limit)

        print("\n" + "=" * 80)
        print(f"ğŸ“ ä¸­æ–‡æ–­å¥æµ‹è¯• - å…± {len(result)} æ®µ (é™åˆ¶: â‰¤{max_limit}å­—/æ®µ)")
        print("=" * 80)
        for i, seg in enumerate(result, 1):
            word_count = count_words(seg)
            status = "âœ“" if word_count <= max_limit else "âœ—"
            print(f"  {status} æ®µ{i:2d} [{word_count:2d}å­—] {seg}")
        print("=" * 80)

        # éªŒè¯ç»“æœ
        assert len(result) > 0, "åº”è¯¥è¿”å›è‡³å°‘ä¸€ä¸ªåˆ†æ®µ"
        assert "".join(result).replace(" ", "") == text.replace(
            " ", ""
        ), "åˆå¹¶ååº”è¯¥ç­‰äºåŸæ–‡"

        # éªŒè¯æ¯æ®µé•¿åº¦
        for seg in result:
            assert count_words(seg) <= max_limit * 1.2, f"åˆ†æ®µè¿‡é•¿: {seg}"

    def test_split_english_text(self, check_env_vars: Callable):
        """Test splitting English text with LLM."""
        check_env_vars("OPENAI_BASE_URL", "OPENAI_API_KEY")

        text = "the upgraded claude sonnet is now available for all users developers can build with the computer use beta on the anthropic api amazon bedrock and google cloud's vertex ai the new claude haiku will be released later this month"
        model = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
        max_limit = 12

        result = split_by_llm(text, model=model, max_word_count_english=max_limit)

        print("\n" + "=" * 80)
        print(f"ğŸ“ è‹±æ–‡æ–­å¥æµ‹è¯• - å…± {len(result)} æ®µ (é™åˆ¶: â‰¤{max_limit} words/æ®µ)")
        print("=" * 80)
        for i, seg in enumerate(result, 1):
            word_count = count_words(seg)
            status = "âœ“" if word_count <= max_limit else "âœ—"
            print(f"  {status} æ®µ{i:2d} [{word_count:2d} words] {seg}")
        print("=" * 80)

        # éªŒè¯ç»“æœ
        assert len(result) > 0, "åº”è¯¥è¿”å›è‡³å°‘ä¸€ä¸ªåˆ†æ®µ"

        # éªŒè¯æ¯æ®µé•¿åº¦
        for seg in result:
            assert count_words(seg) <= max_limit * 1.2, f"åˆ†æ®µè¿‡é•¿: {seg}"

    def test_split_mixed_text(self, check_env_vars: Callable):
        """Test splitting mixed Chinese-English text with LLM."""
        check_env_vars("OPENAI_BASE_URL", "OPENAI_API_KEY")

        text = "ä»Šå¤©æˆ‘ä»¬æ¥ä»‹ç»Claude AIå®ƒæ˜¯ç”±Anthropicå…¬å¸å¼€å‘çš„å¤§è¯­è¨€æ¨¡å‹the model can understand and generate text in multiple languagesåŒ…æ‹¬ä¸­æ–‡å’Œè‹±æ–‡"
        model = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
        max_limit = 15

        result = split_by_llm(text, model=model, max_word_count_cjk=max_limit)

        print("\n" + "=" * 80)
        print(f"ğŸ“ ä¸­è‹±æ··åˆæ–­å¥æµ‹è¯• - å…± {len(result)} æ®µ (é™åˆ¶: â‰¤{max_limit}/æ®µ)")
        print("=" * 80)
        for i, seg in enumerate(result, 1):
            word_count = count_words(seg)
            status = "âœ“" if word_count <= max_limit else "âœ—"
            print(f"  {status} æ®µ{i:2d} [{word_count:2d}] {seg}")
        print("=" * 80)

        # éªŒè¯ç»“æœ
        assert len(result) > 0, "åº”è¯¥è¿”å›è‡³å°‘ä¸€ä¸ªåˆ†æ®µ"

    def test_split_preserves_content(self, check_env_vars: Callable):
        """Test that splitting preserves original content."""
        check_env_vars("OPENAI_BASE_URL", "OPENAI_API_KEY")

        text = "äººå·¥æ™ºèƒ½æŠ€æœ¯æ­£åœ¨æ”¹å˜ä¸–ç•Œå®ƒè®©æˆ‘ä»¬çš„ç”Ÿæ´»å˜å¾—æ›´åŠ ä¾¿åˆ©"
        model = os.getenv("OPENAI_MODEL", "gpt-4o-mini")

        result = split_by_llm(text, model=model)

        # åˆå¹¶ååº”è¯¥å®Œå…¨ç­‰äºåŸæ–‡ï¼ˆå¿½ç•¥ç©ºæ ¼ï¼‰
        merged = "".join(result)
        assert merged.replace(" ", "") == text.replace(" ", ""), "å†…å®¹ä¸åº”è¢«ä¿®æ”¹"

    def test_split_short_text(self, check_env_vars: Callable):
        """Test splitting very short text."""
        check_env_vars("OPENAI_BASE_URL", "OPENAI_API_KEY")

        text = "ä½ å¥½ä¸–ç•Œ"
        model = os.getenv("OPENAI_MODEL", "gpt-4o-mini")

        result = split_by_llm(text, model=model)

        print(f"\nğŸ“ çŸ­æ–‡æœ¬æ–­å¥ç»“æœ: {result}")

        # çŸ­æ–‡æœ¬å¯èƒ½ä¸éœ€è¦åˆ†æ®µ
        assert len(result) >= 1, "è‡³å°‘åº”è¯¥è¿”å›åŸæ–‡æœ¬"
        assert "".join(result).replace(" ", "") == text.replace(" ", "")

    def test_agent_loop_correction(self, check_env_vars: Callable):
        """Test that agent loop can correct errors through feedback."""
        check_env_vars("OPENAI_BASE_URL", "OPENAI_API_KEY")

        # ä½¿ç”¨ä¸€æ®µéœ€è¦åˆ†å¤šæ®µçš„é•¿æ–‡æœ¬
        text = "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯å®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ æ¨¡å¼æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é¢†åŸŸå®ƒä½¿ç”¨ç¥ç»ç½‘ç»œæ¥å¤„ç†å¤æ‚çš„æ•°æ®"
        model = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
        max_limit = 7

        result = split_by_llm(text, model=model, max_word_count_cjk=max_limit)

        print("\n" + "=" * 80)
        print(
            f"ğŸ”„ Agent Loop è‡ªæˆ‘ä¿®æ­£æµ‹è¯• - å…± {len(result)} æ®µ (é™åˆ¶: â‰¤{max_limit}å­—/æ®µ)"
        )
        print("=" * 80)
        for i, seg in enumerate(result, 1):
            word_count = count_words(seg)
            status = "âœ“" if word_count <= max_limit else "âœ—"
            print(f"  {status} æ®µ{i:2d} [{word_count:2d}å­—] {seg}")
        print("=" * 80)

        # éªŒè¯ç»“æœç¬¦åˆè¦æ±‚
        assert len(result) > 1, "åº”è¯¥åˆ†æˆå¤šæ®µ"

        for seg in result:
            word_count = count_words(seg)
            assert (
                word_count <= max_limit * 1.2
            ), f"åˆ†æ®µé•¿åº¦åº”è¯¥ç¬¦åˆé™åˆ¶: {word_count} > {max_limit}"
